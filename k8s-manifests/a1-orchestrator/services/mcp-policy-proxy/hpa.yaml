apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: mcp-policy-proxy
  namespace: a1-orchestrator
  labels:
    app.kubernetes.io/name: mcp-policy-proxy
    app.kubernetes.io/part-of: a1-orchestrator
    app.kubernetes.io/component: policy-proxy
  annotations:
    argocd.argoproj.io/sync-wave: "4"
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: mcp-policy-proxy
  minReplicas: 2
  maxReplicas: 8
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 70
    - type: Resource
      resource:
        name: memory
        target:
          type: Utilization
          averageUtilization: 80
  behavior:
    scaleUp:
      stabilizationWindowSeconds: 300
      policies:
        - type: Percent
          value: 100
          periodSeconds: 15
        - type: Pods
          value: 2
          periodSeconds: 60
      selectPolicy: Max
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
        - type: Percent
          value: 50
          periodSeconds: 60
      selectPolicy: Min

---
apiVersion: keda.sh/v1alpha1
kind: ScaledObject
metadata:
  name: mcp-policy-proxy-keda
  namespace: a1-orchestrator
  labels:
    app.kubernetes.io/name: mcp-policy-proxy
    app.kubernetes.io/part-of: a1-orchestrator
    app.kubernetes.io/component: policy-proxy
  annotations:
    argocd.argoproj.io/sync-wave: "4"
spec:
  scaleTargetRef:
    name: mcp-policy-proxy
  pollingInterval: 30
  cooldownPeriod: 300
  idleReplicaCount: 2
  minReplicaCount: 2
  maxReplicaCount: 12
  fallback:
    failureThreshold: 3
    replicas: 4
  advanced:
    restoreToOriginalReplicaCount: true
    horizontalPodAutoscalerConfig:
      name: mcp-policy-proxy-keda-hpa
      behavior:
        scaleUp:
          stabilizationWindowSeconds: 60
          policies:
            - type: Percent
              value: 100
              periodSeconds: 15
        scaleDown:
          stabilizationWindowSeconds: 300
          policies:
            - type: Percent
              value: 50
              periodSeconds: 60
  triggers:
    # RPS-based scaling
    - type: prometheus
      metadata:
        serverAddress: http://prometheus-operated.monitoring.svc.cluster.local:9090
        metricName: http_requests_per_second
        threshold: '50'
        query: |
          sum(rate(http_requests_total{
            namespace="a1-orchestrator",
            service="mcp-policy-proxy"
          }[2m]))
    
    # P95 latency-based scaling  
    - type: prometheus
      metadata:
        serverAddress: http://prometheus-operated.monitoring.svc.cluster.local:9090
        metricName: http_p95_latency_seconds
        threshold: '0.8'
        query: |
          histogram_quantile(0.95,
            sum(rate(http_request_duration_seconds_bucket{
              namespace="a1-orchestrator",
              service="mcp-policy-proxy"
            }[5m])) by (le)
          )
    
    # Queue depth scaling
    - type: prometheus
      metadata:
        serverAddress: http://prometheus-operated.monitoring.svc.cluster.local:9090
        metricName: policy_queue_depth
        threshold: '25'
        query: |
          sum(policy_evaluation_queue_size{
            namespace="a1-orchestrator",
            service="mcp-policy-proxy"
          })
    
    # Active connections scaling
    - type: prometheus
      metadata:
        serverAddress: http://prometheus-operated.monitoring.svc.cluster.local:9090
        metricName: active_connections
        threshold: '100'
        query: |
          sum(active_connections{
            namespace="a1-orchestrator",
            service="mcp-policy-proxy"
          })